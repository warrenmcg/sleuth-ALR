% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/run_sleuth.R
\name{make_lr_sleuth_object}
\alias{make_lr_sleuth_object}
\title{Make Compositional Sleuth Object}
\usage{
make_lr_sleuth_object(sample_to_covariates,
  full_model = stats::formula("~condition"), target_mapping, beta,
  null_model = stats::formula("~1"), run_models = TRUE,
  aggregate_column = NULL, num_cores = parallel::detectCores() - 2,
  lr_type = "alr", denom_name = NULL, which_var = "obs_tpm",
  lr_method = "both", denom_method = "geomean",
  impute_method = "multiplicative", delta = NULL,
  impute_proportion = 0.65, base = "e", ...)
}
\arguments{
\item{sample_to_covariates, }{the sample_to_covariates matrix for sleuth}

\item{full_model, }{the full model for sleuth}

\item{target_mapping, }{the target mapping data frame for sleuth}

\item{beta, }{the beta you wish to use for the Wald test. If NULL,
the Wald test will be skipped.}

\item{null_model, }{the null model to be the baseline for the LR test.
default is the intercept only model (~1).}

\item{run_models}{boolean to see if the modeling step should be done.
If \code{FALSE}, only sleuth_prep is done. Default is \code{TRUE}.}

\item{aggregate_column, }{character indicating the column in
\code{target_mapping} to be used for "gene-level" sleuth analysis.}

\item{num_cores, }{the number of cores to be used for sleuth analysis}

\item{lr_type, }{either "alr" or "clr" ("ALR" / "CLR" also accepted),
indicating additive logratio or centered logratio transformation}

\item{denom_name, }{target ID names or index numbers of denominators;
required for the ALR transformation. If 'clr' or 'iqlr' is used, this overrides
'lr_type' and the CLR / IQLR transformation will be used. If lr_type is 'clr' or 'iqlr',
this argument will be ignored. If 'best' is used, then the internal function
choose_best_denoms will be used to identify the feature(s) with the most consistent
abundance across all samples in the experiment. See \link{choose_denom} for extra
options.}

\item{which_var, }{must be "obs_tpm" or "obs_counts", to indicate
whether sleuth should model TPMs or estimated counts, respectively}

\item{lr_method}{the choice of how to conduct compositional normalization.
"both" provides a compositional normalization and compositional transformation functions;
"transform" provides a compositional transformation function that also does the normalization}

\item{denom_method}{the choice of what kind of compositional normalization to do
when more than one feature is used. "geomean" takes the geometric of all features within a sample
as the size factor, and "DESeq2" takes the median ratio of a feature to its geometric
mean across all samples}

\item{impute_method}{the choice of how to impute the rounded zeros.
only "multiplicative" and "additive" is supported at this time.}

\item{delta}{a number that is the imputed value during the transformation, to
avoid zeros. If \code{NULL}, delta = impute_proportion * (minimum value in sample)}

\item{impute_proportion}{percentage of minimum value that
becomes the imputed value. Only used if delta is \code{NULL}}

\item{base}{the base used for the logarithm. Currently only supports
"e" or "2" (can also specify the number 2).}

\item{...}{extra options that will be passed on the sleuth.
you can specify here whether \code{read_bootstrap_tpm} and
\code{extra_bootstrap_summary} should be \code{FALSE} (default \code{TRUE}).}
}
\value{
a sleuth object that has been prepped and fitted using the
  full and null models. It will also run the Wald test (if applicable)
  and the LR test
}
\description{
This is a wrapper function for \code{sleuth_prep} that applies
the compositional data analysis approach. Many of the arguments
are ones that will be input into \code{sleuth_prep}.
}
